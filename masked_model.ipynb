{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayeonku/Desktop/CityU/FYP/fyp-dev/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import torch\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Initialize BERT vocabulary...\n",
      "Initialize BERT model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "print('Initialize BERT vocabulary...')\n",
    "bert_tokenizer = BertTokenizer(vocab_file='data/BERT_model_reddit/vocab.txt')\n",
    "print('Initialize BERT model...')\n",
    "bert_model = BertForMaskedLM.from_pretrained('data/BERT_model_reddit').to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Masked Language Model '''\n",
    "def MLM(sgs, input_keywords, thres=1, filter_uninformative=1): # sgs = list of masked sentence\n",
    "    def to_bert_input(tokens, bert_tokenizer):\n",
    "        token_idx = torch.tensor(bert_tokenizer.convert_tokens_to_ids(tokens)) # converts string token to int id and then covert to tensor (type: int)\n",
    "        sep_idx = tokens.index('[SEP]')\n",
    "        segment_idx = token_idx * 0 # initialize to have the same size as token_idx (fill with 0)\n",
    "        segment_idx[(sep_idx + 1):] = 1 # SEP이 나오고 다음 토큰 부터 segment_idx = 1\n",
    "        mask = (token_idx != 0) # token_idx가 0이 아니면 mask=True\n",
    "        return token_idx.unsqueeze(dim = 0).to(device), segment_idx.unsqueeze(dim = 0).to(device), mask.unsqueeze(dim = 0).to(device) # increase dimension then put the model into cpu\n",
    "\n",
    "    def single_MLM(message):\n",
    "        MLM_k = 50\n",
    "        tokens = bert_tokenizer.tokenize(message)\n",
    "        if len(tokens) == 0:\n",
    "            return []\n",
    "        if tokens[0] != CLS:\n",
    "            tokens = [CLS] + tokens\n",
    "        if tokens[-1] != SEP:\n",
    "            tokens.append(SEP)\n",
    "        token_idx, segment_idx, mask = to_bert_input(tokens, bert_tokenizer)\n",
    "        with torch.no_grad(): # stop auto gradient tracking cuz no backpropagation\n",
    "            logits = bert_model(token_idx, segment_idx, mask, masked_lm_labels=None)\n",
    "        logits = logits.squeeze(0) # remove 1 in index 0 in dimension\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if token == MASK:\n",
    "                topk_prob, topk_indices = torch.topk(probs[idx, :], MLM_k) # topk는 tensor에서 값이 가장 큰 k(MLM_k) 개를 추출하는 연산\n",
    "                # .cpu() = GPU 메모리에 올려져 있는 tensor를 cpu 메모리로 복사하는 method\n",
    "                # tensor를 numpy로 변환하여 반환. 이때 저장공간을 공유하기 때문에 하나를 변경하면 다른 하나도 변경된다.\n",
    "                # 또한 cpu 메모리에 올려져 있는 tensor만 .numpy() method를 사용할 수 있다\n",
    "                topk_tokens = bert_tokenizer.convert_ids_to_tokens(topk_indices.cpu().numpy())\n",
    "\n",
    "        out = [[topk_tokens[i], float(topk_prob[i])] for i in range(MLM_k)] # returns word and prob\n",
    "        return out\n",
    "    \n",
    "    PAD, MASK, CLS, SEP = '[PAD]', '[MASK]', '[CLS]', '[SEP]'\n",
    "    MLM_score = defaultdict(float)\n",
    "    temp = sgs if len(sgs) < 10 else tqdm(sgs) # 문장 갯수가 10보다 작으면 그대로 지정, 아니면 tqdm(list)로 지정\n",
    "    skip_ms_num = 0\n",
    "    good_sgs = []\n",
    "    for sgs_i in temp: # sgs_i = each sentence\n",
    "        try:\n",
    "            top_words = single_MLM(sgs_i) # topk word랑 그 %를 top MLM_k개 리스트로\n",
    "            seen_input = 0\n",
    "            for input_i in input_keywords: # for each formal drug name\n",
    "                if input_i in [x[0] for x in top_words[:thres]]: # in this case, thres=5; x[0] = topk word\n",
    "                    seen_input += 1 # formal drug name이 5개의 top words 중에 있으면 seen_input+=1\n",
    "            if filter_uninformative == 1 and seen_input < 2:\n",
    "                skip_ms_num += 1\n",
    "                continue\n",
    "            good_sgs.append(sgs_i)\n",
    "            for j in top_words:\n",
    "                if j[0] in string.punctuation: # j[0]=key of top_words=topk word\n",
    "                    continue\n",
    "                if j[0] in stopwords.words('english'):\n",
    "                    continue\n",
    "                if j[0] in input_keywords:\n",
    "                    continue\n",
    "                if j[0][:2] == '##':  # the '##' by BERT indicates that is not a word.\n",
    "                    continue\n",
    "                MLM_score[j[0]] += j[1]\n",
    "            # print(sgs_i)\n",
    "            # print([x[0] for x in top_words[:20]])\n",
    "        except KeyError:\n",
    "            print(\"KeyError occurred in MLM, and the sentence is: \", sgs_i)\n",
    "    out = sorted(MLM_score, key=lambda x: MLM_score[x], reverse=True) # sort MLM_score based on the value (descending order)\n",
    "    out_tuple = [[x, MLM_score[x]] for x in out]\n",
    "    if len(sgs) >= 10:\n",
    "        print('The percentage of uninformative masked sentences is {:d}/{:d} = {:.2f}%'.format(skip_ms_num, len(sgs), float(skip_ms_num)/len(sgs)*100))\n",
    "    return out, out_tuple, good_sgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' topk part '''\n",
    "def euphemism_detection(input_keywords, all_text, ms_limit, filter_uninformative): # input_keywords = drug formal\n",
    "    print('\\n' + '*' * 40 + ' [Euphemism Detection] ' + '*' * 40)\n",
    "    print('[util.py] Input Keyword: ', end='')\n",
    "    print(input_keywords)\n",
    "    print('[util.py] Extracting masked sentences for input keywords...')\n",
    "    masked_sentence = []\n",
    "    for sentence in tqdm(all_text):\n",
    "        temp = nltk.word_tokenize(sentence)\n",
    "        for input_keyword_i in input_keywords:\n",
    "            if input_keyword_i not in temp:\n",
    "                continue\n",
    "            temp_index = temp.index(input_keyword_i)\n",
    "            masked_sentence += [' '.join(temp[: temp_index]) + ' [MASK] ' + ' '.join(temp[temp_index + 1:])] # ['a b [MASK] d e f']\n",
    "    random.shuffle(masked_sentence)\n",
    "    masked_sentence = masked_sentence[:ms_limit]\n",
    "    print('[util.py] Generating top candidates...')\n",
    "    top_words, _, informative = MLM(masked_sentence, input_keywords, thres=5, filter_uninformative=filter_uninformative) # top_words(=MLM_score) = dictionary of MLM_score in descending ordered value\n",
    "    return top_words, informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Printing functions '''\n",
    "class print_color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "\n",
    "def color_print_top_words(top_words, gt_euphemism):\n",
    "    print('[Euphemism Candidates]: ')\n",
    "    gt_euphemism_upper = set([y for x in gt_euphemism for y in x.split()])\n",
    "    for i in top_words[:30]: # change this number for top k\n",
    "        if i in gt_euphemism:\n",
    "            print(print_color.BOLD + print_color.PURPLE + i + print_color.END, end=', ')\n",
    "        # elif i in gt_euphemism_upper:\n",
    "        #     print(print_color.UNDERLINE + print_color.PURPLE + i + print_color.END, end=', ')\n",
    "        else:\n",
    "            print(i, end=', ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Evaluation '''\n",
    "def evaluate_detection(top_words, gt_euphemism): # top_words = from euphemism_detection fnc, gt_euphemism = dictionary {euph:[formal drug names related]}\n",
    "    color_print_top_words(top_words, gt_euphemism)\n",
    "    correct_list = []  # appear in the ground truth\n",
    "    correct_list_upper = []  # not appear in the ground truth but contain in a ground truth phase.\n",
    "    gt_euphemism_upper = set([y for x in gt_euphemism for y in x.split()]) # phrases split into words, only unique words saved\n",
    "    for i, x in enumerate(top_words):\n",
    "        correct_list.append(1 if x in gt_euphemism else 0) # if name from formal drug names appear in top_wrods = 1, else 0\n",
    "        correct_list_upper.append(1 if x in gt_euphemism_upper else 0) # 얘는 formal drug name을 단어별로 분해한 버전\n",
    "\n",
    "    topk_precision_list = []\n",
    "    cummulative_sum = 0\n",
    "    topk_precision_list_upper = []\n",
    "    cummulative_sum_upper = 0\n",
    "    for i in range(0, len(correct_list)):\n",
    "        cummulative_sum += correct_list[i] # add 1 for every correct word\n",
    "        topk_precision_list.append(cummulative_sum/(i+1))\n",
    "        cummulative_sum_upper += correct_list_upper[i]\n",
    "        topk_precision_list_upper.append(cummulative_sum_upper/(i+1))\n",
    "\n",
    "    # i   correct_list    cummulative_sum     topk_precision_list     \n",
    "    # 0   1               1                   1/1 = 1                \n",
    "    # 1   0               1                   1/2 = 0.5               \n",
    "    # 2   0               1                   1/3 = 0.3333\n",
    "    # 3   1               2                   2/4 = 0.5\n",
    "\n",
    "    # print precision value\n",
    "    for topk in [10, 20, 30, 50]:\n",
    "        if topk < len(topk_precision_list): # topk가 len보다 작아야 출력 가능하니까 놓은 if\n",
    "            print('Top-{:d} precision is {:.2f}'.format(topk, topk_precision_list[topk-1]))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[read_data.py] Reading data with read_all_data...\n",
      "[read_data.py] Reading data with read_files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6174234/6174234 [01:37<00:00, 63005.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[read_data.py] Finish reading data using 99.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''' REAL main '''\n",
    "\n",
    "# from identification import euphemism_identification\n",
    "from read_files import read_all_data\n",
    "\n",
    "dataset = 'data/output/processed_corpus.txt'\n",
    "euph_file = 'data/euphemism_answer_drug.txt'\n",
    "t_file = 'data/target_keywords_drug.txt'\n",
    "auto_file = 'data/AutoPhrase.txt'\n",
    "''' Read Data '''\n",
    "all_text, euphemism_answer, drug_formal, target_name, _ = read_all_data(dataset, euph_file, t_file, auto_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************** [Euphemism Detection] ****************************************\n",
      "[util.py] Input Keyword: ['acetaminophen and oxycodone combination', 'adderall', 'alprazolam', 'amphetamine', 'amphetamine and dextroamphetamine combination', 'buprenorphine and naloxone combination', 'clonazepam', 'cocaine', 'concerta', 'crack cocaine', 'daytrana', 'dilaudid', 'ecstasy', 'fentanyl', 'flunitrazepam', 'gamma-hydroxybutyric acid', 'ghb', 'hash oil', 'heroin', 'hydrocodone', 'hydromorphone', 'ketalar', 'ketamine', 'khat', 'klonopin', 'lorcet', 'lsd', 'lysergic acid diethylamide', 'marijuana', 'marijuana concentrates', 'mdma', 'mescaline', 'methamphetamine', 'methylphenidate', 'molly', 'morphine', 'norco', 'opium', 'oxaydo', 'oxycodone', 'oxycontin', 'pcp', 'percocet', 'peyote', 'phencyclidine', 'promethazine', 'psilocybin mushrooms', 'ritalin', 'rohypnol', 'roxicodone', 'steroids', 'suboxone', 'synthetic cannabinoids', 'synthetic cathinones', 'u-47700', 'vicodin', 'xanax']\n",
      "[util.py] Extracting masked sentences for input keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246263/246263 [00:29<00:00, 8312.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[util.py] Generating top candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:26<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of uninformative masked sentences is 1827/2000 = 91.35%\n",
      "[Euphemism Candidates]: \n",
      "\u001b[1m\u001b[95mweed\u001b[0m, \u001b[1m\u001b[95macid\u001b[0m, drug, alcohol, \u001b[1m\u001b[95mcoke\u001b[0m, cannabis, drugs, mushrooms, \u001b[1m\u001b[95mspeed\u001b[0m, md, pills, crack, \u001b[1m\u001b[95mpot\u001b[0m, something, \u001b[1m\u001b[95mpowder\u001b[0m, \u001b[1m\u001b[95mblow\u001b[0m, \u001b[1m\u001b[95mk\u001b[0m, \u001b[1m\u001b[95mcrystal\u001b[0m, tobacco, \u001b[1m\u001b[95mlucy\u001b[0m, \u001b[1m\u001b[95mlean\u001b[0m, \u001b[1m\u001b[95mhash\u001b[0m, l, \u001b[1m\u001b[95mh\u001b[0m, \u001b[1m\u001b[95mstuff\u001b[0m, \u001b[1m\u001b[95mhydro\u001b[0m, \u001b[1m\u001b[95me\u001b[0m, \u001b[1m\u001b[95mcactus\u001b[0m, psychedelic, cigarettes, \n",
      "Top-10 precision is 0.40\n",
      "Top-20 precision is 0.50\n",
      "Top-30 precision is 0.57\n",
      "Top-50 precision is 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''' Euphemism Detection '''\n",
    "top_words, informative = euphemism_detection(drug_formal, all_text, ms_limit=2000, filter_uninformative=1)\n",
    "evaluate_detection(top_words, euphemism_answer)\n",
    "\n",
    "with open('data/output/viz_informative_sentence.txt', 'w') as fout:\n",
    "    for sent in informative:\n",
    "        line = f\"{sent}\\n\"\n",
    "        fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf42840c5253c66635351366f71543fb19083080d676bbd340381a7827cbdf14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
